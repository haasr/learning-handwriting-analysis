{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b100966-23e0-44c1-b696-adaa6c8e3a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import StringLookup\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "try: [tf.config.experimental.set_memory_growth(gpu, True) for gpu in tf.config.experimental.list_physical_devices(\"GPU\")]\n",
    "except: pass\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169829d2-6d70-499a-b1ae-e9362a0c51ab",
   "metadata": {},
   "source": [
    "## Dataset Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c64f6b7f-1d59-4c83-8850-9038cc88877b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(words_list) = 96456\n",
      "words_list[0:10]: ['r06-076-07-06 ok 177 1807 2010 76 53 CC or\\n', 'n01-004-01-01 ok 180 614 906 246 69 JJ unable\\n', 'g06-011f-00-03 ok 203 778 721 46 70 INO of\\n', 'f04-011-07-01 ok 145 504 1976 118 78 BEDZ was\\n', 'e04-103-01-01 ok 174 471 916 205 123 VB plank\\n', 'g06-047g-04-05 ok 182 924 1430 193 67 NP Europe\\n', 'm06-056-04-11 ok 158 2061 1537 11 21 , ,\\n', 'j06-026-03-04 ok 185 1593 1416 341 129 NN sunlight\\n', 'm06-019-01-12 ok 189 1837 949 142 50 CD three\\n', 'a04-043-02-05 ok 186 1906 1113 59 68 INO of\\n']\n"
     ]
    }
   ],
   "source": [
    "base_path = \"Datasets/IAM_Words/\"\n",
    "words_list = []\n",
    "\n",
    "words = None\n",
    "with open(f\"{base_path}/words.txt\", 'r') as f:\n",
    "    words = f.readlines()\n",
    "\n",
    "for line in words:\n",
    "    if line[0] == \"#\": continue\n",
    "    if line.split(\" \")[1] != 'err': words_list.append(line) # Append correctly predicted word\n",
    "\n",
    "print(f\"len(words_list) = {len(words_list)}\")\n",
    "np.random.shuffle(words_list)\n",
    "print(f\"words_list[0:10]: {words_list[0:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3344b82-50e6-4216-a197-c1e06fbc2399",
   "metadata": {},
   "source": [
    "### Train-validation-test split (90:5:5 ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0d521761-96fb-4774-ba68-083e99b322f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training samples: 86810\n",
      "Total validation samples: 4823\n",
      "Total test samples: 4823\n"
     ]
    }
   ],
   "source": [
    "split_idx = int(0.9*len(words_list))\n",
    "train_samples = words_list[:split_idx]\n",
    "\n",
    "remaining = words_list[split_idx:]\n",
    "half_idx = int(0.5*len(remaining))\n",
    "validation_samples = remaining[:half_idx]\n",
    "test_samples = remaining[half_idx:]\n",
    "\n",
    "assert (len(train_samples) + len(validation_samples) + len(test_samples)) == len(words_list)\n",
    "\n",
    "print(f\"Total training samples: {len(train_samples)}\")\n",
    "print(f\"Total validation samples: {len(validation_samples)}\")\n",
    "print(f\"Total test samples: {len(test_samples)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8fbf4bd-d747-4674-b307-e8c2dcb26881",
   "metadata": {},
   "source": [
    "## Data Input Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f82397-d65b-411c-b53a-8f456ed183e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
