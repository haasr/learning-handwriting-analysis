{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b100966-23e0-44c1-b696-adaa6c8e3a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import StringLookup\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "try: [tf.config.experimental.set_memory_growth(gpu, True) for gpu in tf.config.experimental.list_physical_devices(\"GPU\")]\n",
    "except: pass\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169829d2-6d70-499a-b1ae-e9362a0c51ab",
   "metadata": {},
   "source": [
    "## I. Dataset Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c64f6b7f-1d59-4c83-8850-9038cc88877b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(words_list) = 96456\n",
      "words_list[0:10]: ['r06-076-07-06 ok 177 1807 2010 76 53 CC or\\n', 'n01-004-01-01 ok 180 614 906 246 69 JJ unable\\n', 'g06-011f-00-03 ok 203 778 721 46 70 INO of\\n', 'f04-011-07-01 ok 145 504 1976 118 78 BEDZ was\\n', 'e04-103-01-01 ok 174 471 916 205 123 VB plank\\n', 'g06-047g-04-05 ok 182 924 1430 193 67 NP Europe\\n', 'm06-056-04-11 ok 158 2061 1537 11 21 , ,\\n', 'j06-026-03-04 ok 185 1593 1416 341 129 NN sunlight\\n', 'm06-019-01-12 ok 189 1837 949 142 50 CD three\\n', 'a04-043-02-05 ok 186 1906 1113 59 68 INO of\\n']\n"
     ]
    }
   ],
   "source": [
    "base_path = \"Datasets/IAM_Words/\"\n",
    "words_list = []\n",
    "\n",
    "words = None\n",
    "with open(f\"{base_path}/words.txt\", 'r') as f:\n",
    "    words = f.readlines()\n",
    "\n",
    "for line in words:\n",
    "    if line[0] == \"#\": continue\n",
    "    if line.split(\" \")[1] != 'err': words_list.append(line) # Append correctly predicted word\n",
    "\n",
    "print(f\"len(words_list) = {len(words_list)}\")\n",
    "np.random.shuffle(words_list)\n",
    "print(f\"words_list[0:10]: {words_list[0:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3344b82-50e6-4216-a197-c1e06fbc2399",
   "metadata": {},
   "source": [
    "### Train-validation-test split (90:5:5 ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0d521761-96fb-4774-ba68-083e99b322f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training samples: 86810\n",
      "Total validation samples: 4823\n",
      "Total test samples: 4823\n"
     ]
    }
   ],
   "source": [
    "split_idx = int(0.9*len(words_list))\n",
    "train_samples = words_list[:split_idx]\n",
    "\n",
    "remaining = words_list[split_idx:]\n",
    "half_idx = int(0.5*len(remaining))\n",
    "validation_samples = remaining[:half_idx]\n",
    "test_samples = remaining[half_idx:]\n",
    "\n",
    "assert (len(train_samples) + len(validation_samples) + len(test_samples)) == len(words_list)\n",
    "\n",
    "print(f\"Total training samples: {len(train_samples)}\")\n",
    "print(f\"Total validation samples: {len(validation_samples)}\")\n",
    "print(f\"Total test samples: {len(test_samples)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8fbf4bd-d747-4674-b307-e8c2dcb26881",
   "metadata": {},
   "source": [
    "## II. Data Input Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d6b834-11f5-40d4-837a-09d6674022a2",
   "metadata": {},
   "source": [
    "Start by preparing the image paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fd1174a4-180c-4ea4-8ff8-27c989fc65dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets/IAM_Words/words\n"
     ]
    }
   ],
   "source": [
    "base_image_path = os.path.join(base_path, \"words\")\n",
    "print(base_image_path)\n",
    "\n",
    "def get_image_paths_and_labels(samples):\n",
    "    paths = []\n",
    "    corrected_samples = []\n",
    "\n",
    "    for i, file_line in enumerate(samples):\n",
    "        line_split = file_line.strip().split(' ')\n",
    "\n",
    "        # Image path format:\n",
    "        # base_image_path/part1-part2/part1-part2-part3.png\n",
    "        img_name = line_split[0]\n",
    "        name_split = img_name.split('-')\n",
    "        part1 = name_split[0]\n",
    "        part2 = name_split[1]\n",
    "        img_path = os.path.join(\n",
    "            base_image_path, part1, f\"{part1}-{part2}\", f\"{img_name}.png\"\n",
    "        )\n",
    "        if os.path.getsize(img_path):\n",
    "            paths.append(img_path) # Append if path is actually a valid file\n",
    "            corrected_samples.append(file_line.split('\\n')[0])\n",
    "    \n",
    "    return paths, corrected_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0ec9571c-b2e4-4c34-8245-0ebe6f00ab7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img_paths, train_labels = get_image_paths_and_labels(train_samples)\n",
    "validation_img_paths, validation_labels = get_image_paths_and_labels(validation_samples)\n",
    "test_img_paths, test_labels = get_image_paths_and_labels(test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "994bdc97-93af-497d-bd45-8ef97ca5ac06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['r06-076-07-06 ok 177 1807 2010 76 53 CC or',\n",
       " 'n01-004-01-01 ok 180 614 906 246 69 JJ unable',\n",
       " 'g06-011f-00-03 ok 203 778 721 46 70 INO of',\n",
       " 'f04-011-07-01 ok 145 504 1976 118 78 BEDZ was',\n",
       " 'e04-103-01-01 ok 174 471 916 205 123 VB plank',\n",
       " 'g06-047g-04-05 ok 182 924 1430 193 67 NP Europe',\n",
       " 'm06-056-04-11 ok 158 2061 1537 11 21 , ,',\n",
       " 'j06-026-03-04 ok 185 1593 1416 341 129 NN sunlight',\n",
       " 'm06-019-01-12 ok 189 1837 949 142 50 CD three',\n",
       " 'a04-043-02-05 ok 186 1906 1113 59 68 INO of']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "58f206ee-392a-4573-9ef0-147f05d4bfb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Datasets/IAM_Words/words/r06/r06-076/r06-076-07-06.png',\n",
       " 'Datasets/IAM_Words/words/n01/n01-004/n01-004-01-01.png',\n",
       " 'Datasets/IAM_Words/words/g06/g06-011f/g06-011f-00-03.png',\n",
       " 'Datasets/IAM_Words/words/f04/f04-011/f04-011-07-01.png',\n",
       " 'Datasets/IAM_Words/words/e04/e04-103/e04-103-01-01.png',\n",
       " 'Datasets/IAM_Words/words/g06/g06-047g/g06-047g-04-05.png',\n",
       " 'Datasets/IAM_Words/words/m06/m06-056/m06-056-04-11.png',\n",
       " 'Datasets/IAM_Words/words/j06/j06-026/j06-026-03-04.png',\n",
       " 'Datasets/IAM_Words/words/m06/m06-019/m06-019-01-12.png',\n",
       " 'Datasets/IAM_Words/words/a04/a04-043/a04-043-02-05.png']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_img_paths[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826ea09f-9082-4e35-afb1-b215658136db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
